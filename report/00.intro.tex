\section{Introduction}

Our goal is to realize a Neural Network model simulator and apply it to the given problem sets: the 3 monks classification problems and the ML cup regression, trying of course to make it as much predictive as possible over the test set.

We implemented a feed-forward multi-layer perceptron supporting standard back propagation and several versions of gradient descent: multi-batched, single-batched, with or without momentum and employing various loss and activation functions.

Our main assumption to predict the ML cup blind test values was that the underlying function was smooth and compact supported, so that we could expect to really learn it using a standard MLP. Moreover observing the data and trying to fit them with various models we realized that output data had been perturbed by some noise and then we paid careful attention not to commit overfitting. 

\section{Method}

\subsection{Code}

We implemented our NN in \texttt{Python 2.7.13} and relied on the \texttt{numpy} library for linear algebra operations that are paramount in back propagation algorithm. We put some effort in keeping our code as much parametric as possible in those features that we deemed as relevant for the model definition so that we would have been able to adapt it along the way.

In file \texttt{network.py} you can find the class Network that is basically a list of Layer class objects and a pair of activation functions (one internal and one for the last layer, often required of a different kind). This class implements feed-forward and propagate-back methods exploiting the homonym Layer methods as building blocks, thus enhancing code modularity. Moreover we defined a class DiffFunction representing a function and its derivative, so that we could easily initialize our networks.

In file \texttt{gradient\_descent.py} we implemented \texttt{gradient\_descent} procedure keeping it parametric in batch size so that we could make more trials over different batch sizes, we implemented also momentum parametric in $\beta$ so that we could tune it or even shut it down during our model development.
Finally we chose to regularize adding to the loss function the squared norm of weights multiplied by $\lambda$ (\texttt{mu} in code, since lambda is a python keyword) as a penalty term. As far as stopping criterion is concerned at a first glance we deferred its implementation and then found it useless since our learning plots just stabilized after 1000 epochs.

In file \texttt{utility.py} we coded 4 different activation functions (\texttt{tanh}, \texttt{softMax}, \texttt{softPlus}, \texttt{reLU}, \texttt{identity}), 4 different loss functions (\texttt{crossEntropy}, \texttt{binaryCrossEntropy}, \texttt{euclideanLoss}, \texttt{squaredLoss}) looking for the best combo, a couple of accuracy evaluators and some I/O formatting related procedures.

\subsection{Preprocessing}

Our first attempt did not encoded monks inputs one-hot and it was way more impeded to achieve the 100\% accuracy, easily conquered later exploiting the suggested preprocessing. We analyzed cup input data finding that they were normalized yet, indeed they are distributed exactly as a normal density of mean 0 and variance 1, so we did not apply any transformation. Cup output was always less than 30 in norm thus we regarded a normalization as useless. Our last model employed a little input preprocessing: we fed not only inputs but even their pairwise products. Apart from that we did not employ any tricky preprocessing as in monks case. 

\subsection{Validation Scheme}

Monks dataset was endowed of an explicit test set to finally assess the model performance, we just separated a random fraction (25\%) of the training set for validation purposes. In cup dataset instead we needed to randomly separate an internal test set (10\%) by ourselves, thus you can find 2 different files attached: cup.test and cup.train. We separated them in the beginning in order to keep test strictly apart. During our cross validation we further split the training set between train (75\%) and validation (25\%). It is worth noting that we did not employ a k-fold approach but sticked to sample a random validation for every trial, then we took the average loss (or accuracy) over those sampling (usually 10, since variance was low).

\subsection{Trials Pursued}

Monks started working with 100\% accuracy immediately after adopting one-hot encoding, we had just to pay some attention to regularization for the 3rd set.
Cup dataset was trickier and it required us to try the heuristics described above (and in deeper details in experiments section) first manually trying to guess the right order of magnitude for parameters and then with a more exhaustive grid search. In the end we put our best (ranked for increasing validation risk) trained network in an ensemble and our final model just outputs the average of their feed-forward outputs. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "report"
%%% End:
